name: Train and Push Model

on:
  workflow_dispatch:  # d√©clenchement manuel

jobs:
  train-and-upload:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'  # version stable

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install huggingface_hub gdown

      - name: Download dataset from Google Drive
        run: |
          gdown --folder https://drive.google.com/drive/u/0/folders/16JHLtiSgbgQTZgiv-MLC_Ryn_mxVvVnN -O dataset

      - name: Train model
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python train.py \
            --xml_dir dataset/plagiarism-annotation \
            --susp_dir dataset/suspicious-documents \
            --src_dir dataset/source-documents \
            --out_dir data \
            --model_out best_model.pth \
            --batch_size 8 \
            --epochs 8 \
            --max_len 128 \
            --augment

      - name: Push model to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          from huggingface_hub import HfApi
          api = HfApi()
          api.upload_file(
            path_or_fileobj='best_model.pth',
            path_in_repo='best_model.pth',
            repo_id='fatehsaidi/arabic-plag-detecteur-model',
            token='${{ secrets.HF_TOKEN }}'
          )
          "
