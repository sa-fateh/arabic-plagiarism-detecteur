{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12908849,"sourceType":"datasetVersion","datasetId":8168065}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# data.py\n\n\"\"\"\nConstruction du dataset CSV pour la détection de plagiat arabe,\navec split par documents, augmentations et option de cross-validation.\n\"\"\"\n\nimport os\nimport glob\nimport random\nimport xml.etree.ElementTree as ET\n\nimport pandas as pd\nfrom sklearn.model_selection import GroupShuffleSplit, GroupKFold\n\nfrom preprocess import extract_fragment\n\ndef build_dataset(\n    xml_dir: str,\n    susp_dir: str,\n    src_dir: str,\n    out_dir: str,\n    augment: bool = True,\n    neg_length: int = 50,\n    test_size: float = 0.30,\n    random_state: int = 42,\n    cv_folds: int = None\n):\n    \"\"\"\n    Construit et exporte :\n      - train.csv, val.csv, test.csv  (split par document)\n      - Optionnellement folds pour cross-validation (train_i.csv / val_i.csv)\n\n    Args:\n        xml_dir     : dossier PAN XML (features annotées)\n        susp_dir    : dossier des textes suspects (.txt)\n        src_dir     : dossier des textes source (.txt)\n        out_dir     : dossier de sortie pour les CSV\n        augment     : active shuffle de mots pour doublement des positifs\n        neg_length  : taille fixe (en caractères) des exemples négatifs\n        test_size   : fraction (val+test) pour GroupShuffleSplit\n        random_state: graine pour reproductibilité\n        cv_folds    : si int>1, génère cross-validation par groupe\n\n    Returns:\n        (train_csv, val_csv, test_csv) ou list de tuples si cv_folds.\n    \"\"\"\n\n    # 1) Extraire paires positives depuis chaque XML\n    records = []\n    xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n    print(f\">>> {len(xml_files)} fichiers XML trouvés dans {xml_dir}\")\n\n    for xml_fp in xml_files:\n        tree = ET.parse(xml_fp)\n        root = tree.getroot()\n        susp_name = os.path.basename(xml_fp).replace(\".xml\", \".txt\")\n        susp_path = os.path.join(susp_dir, susp_name)\n\n        for feat in root.findall(\"feature\"):\n            to   = int(feat.get(\"this_offset\"))\n            tl   = int(feat.get(\"this_length\"))\n            src_ref = feat.get(\"source_reference\")\n            so   = int(feat.get(\"source_offset\"))\n            sl   = int(feat.get(\"source_length\"))\n            src_path = os.path.join(src_dir, src_ref)\n\n            s_frag = extract_fragment(susp_path, to, tl)\n            r_frag = extract_fragment(src_path, so, sl)\n            if not s_frag or not r_frag:\n                continue\n\n            records.append({\n                \"suspicious_reference\": susp_name,\n                \"this_offset\": to,\n                \"this_length\": tl,\n                \"source_reference\": src_ref,\n                \"source_offset\": so,\n                \"source_length\": sl,\n                \"suspicious_text\": s_frag,\n                \"source_text\":    r_frag,\n                \"label\": 1\n            })\n\n    # 2) Data augmentation : shuffle de mots pour positifs\n    if augment:\n        augmented = []\n        for rec in records:\n            ws = rec[\"suspicious_text\"].split()\n            wr = rec[\"source_text\"].split()\n            random.shuffle(ws)\n            random.shuffle(wr)\n            aug = rec.copy()\n            aug[\"suspicious_text\"] = \" \".join(ws)\n            aug[\"source_text\"]     = \" \".join(wr)\n            augmented.append(aug)\n        records.extend(augmented)\n\n    # 3) Génération de négatifs\n    susp_paths = glob.glob(os.path.join(susp_dir, \"*.txt\"))\n    src_paths  = glob.glob(os.path.join(src_dir,  \"*.txt\"))\n    pos_count  = sum(1 for r in records if r[\"label\"] == 1)\n\n    for _ in range(pos_count):\n        sp = random.choice(susp_paths)\n        rp = random.choice(src_paths)\n        s_full = extract_fragment(sp, 0, 10**6)\n        r_full = extract_fragment(rp, 0, 10**6)\n        if len(s_full) < neg_length or len(r_full) < neg_length:\n            continue\n\n        i = random.randint(0, len(s_full) - neg_length)\n        j = random.randint(0, len(r_full) - neg_length)\n        records.append({\n            \"suspicious_reference\": os.path.basename(sp),\n            \"this_offset\": i,\n            \"this_length\": neg_length,\n            \"source_reference\": os.path.basename(rp),\n            \"source_offset\": j,\n            \"source_length\": neg_length,\n            \"suspicious_text\": s_full[i:i+neg_length],\n            \"source_text\":    r_full[j:j+neg_length],\n            \"label\": 0\n        })\n\n    # 4) Passage en DataFrame\n    df = pd.DataFrame(records).reset_index(drop=True)\n    print(f\"→ total examples avant split: {len(df)} (pos={df.label.sum()}, neg={len(df)-df.label.sum()})\")\n\n    # 5) Split train / val / test par document – pas de fuite\n    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    groups = df[\"suspicious_reference\"]\n    train_idx, temp_idx = next(gss.split(df, groups=groups))\n    train_df = df.iloc[train_idx].reset_index(drop=True)\n    temp_df  = df.iloc[temp_idx].reset_index(drop=True)\n\n    gss2 = GroupShuffleSplit(n_splits=1, test_size=0.50, random_state=random_state)\n    groups_temp = temp_df[\"suspicious_reference\"]\n    val_idx, test_idx = next(gss2.split(temp_df, groups=groups_temp))\n    val_df   = temp_df.iloc[val_idx].reset_index(drop=True)\n    test_df  = temp_df.iloc[test_idx].reset_index(drop=True)\n\n    # 6) Sauvegarde des CSV standards\n    os.makedirs(out_dir, exist_ok=True)\n    train_csv = os.path.join(out_dir, \"train.csv\")\n    val_csv   = os.path.join(out_dir, \"val.csv\")\n    test_csv  = os.path.join(out_dir, \"test.csv\")\n\n    train_df.to_csv(train_csv, index=False, encoding=\"utf-8\")\n    val_df.to_csv(val_csv,     index=False, encoding=\"utf-8\")\n    test_df.to_csv(test_csv,   index=False, encoding=\"utf-8\")\n    print(f\"✅ Dataset prêt → train: {len(train_df)} | val: {len(val_df)} | test: {len(test_df)}\")\n\n    # 7) Option cross-validation (par document)\n    cv_splits = []\n    if cv_folds and cv_folds > 1:\n        print(f\">>> Génération de {cv_folds}-fold CV par documents\")\n        gkf = GroupKFold(n_splits=cv_folds)\n        for fold, (tr_idx, vl_idx) in enumerate(gkf.split(df, groups=df[\"suspicious_reference\"])):\n            fold_dir = os.path.join(out_dir, f\"cv_fold_{fold+1}\")\n            os.makedirs(fold_dir, exist_ok=True)\n\n            df.iloc[tr_idx].reset_index(drop=True).to_csv(\n                os.path.join(fold_dir, \"train.csv\"), index=False, encoding=\"utf-8\"\n            )\n            df.iloc[vl_idx].reset_index(drop=True).to_csv(\n                os.path.join(fold_dir, \"val.csv\"),   index=False, encoding=\"utf-8\"\n            )\n            cv_splits.append((fold_dir, tr_idx, vl_idx))\n\n        print(f\"✅ {cv_folds}-fold CV prêt dans {out_dir}/cv_fold_*\")\n\n    # 8) Retourne les chemins CSV\n    return (train_csv, val_csv, test_csv) if not cv_splits else {\n        \"standard\": (train_csv, val_csv, test_csv),\n        \"cv\": cv_splits\n    }","metadata":{"_uuid":"0b672fd9-96b7-4579-aad7-8d6dfa1b24fe","_cell_guid":"c857ea5a-b88f-4dca-8878-73d17070ae08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}